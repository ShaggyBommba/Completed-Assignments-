# -*- coding: utf-8 -*-
"""Assignment 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxdQVO0hUrCf5TtQP6Mcr9750q0HHIp-

# Part 1: Real estate prices

Import necessary libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import plotly.graph_objects as go
import plotly.express as px

"""Importing data from houses.csv


"""

data = pd.read_csv('houses.csv')

"""

*   Compute basic descriptive statistics about the prices (second column)

"""

#Present first five rows in order to find the name of the second column, "52000" is the second column which represents the price of the property
data.head()
data['52000'].describe()

"""

*   Plot a histogram that shows the distribution of the prices

"""

plt.hist(data['52000'])
plt.title('Distribution of property prices')
plt.xlabel('Price')
plt.ylabel('Frequency')

"""As we can see, the histogram is not informative and it is difficult to understand and see the distribution of property prices. This is because the histogram divides the data into equal parts and the majority of the values fall into the first bin (of total 10). The first bin corresponds to property prices between 0 - 4 846 571,7. To solve this problem and make the histogram more informative, we exclude values seen as outliers. Where to draw that line depends on when you think the histogram looks sufficiently informative. We chose to draw the line at 95% because we still want to show as much data as possible, while we have a histogram that shows the distribution in a clear way. By excluding values above 95% we are given the following histogram: """

d = data[data['52000'] < data['52000'].quantile(0.95)] 
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Distribution of property prices')
plt.hist(d['52000'])

"""

*   Is real estate more expensive in London?

"""

inside_london = data[data['DERBYSHIRE'].str.contains('LONDON')]
outside_london = data[~data.DERBYSHIRE.str.contains('LONDON')]

insideQ3 = inside_london['52000'].quantile(0.95)
outsideQ3 = outside_london['52000'].quantile(0.95)

new_inside = inside_london[inside_london['52000'] < insideQ3]
new_outside = outside_london[outside_london['52000'] < outsideQ3]

fig, (ax1, ax2) = plt.subplots(2, sharex=True)
ax1.hist(new_inside['52000'], bins=20)
ax1.set(ylabel='Frequency', title='Inside London')
ax2.hist(new_outside['52000'], bins=20)
ax2.set(xlabel='Price', ylabel='Frequency', title='Outside London')

plt.tight_layout()

"""As we did above, outliers that lie above 95% have been excluded in order to make the comparison more informative. As we can see, real estate is more expensive in London compared to outside London. Inside London, 95% stretches above 700 000 while outside London 95% is slightly under 400 000. We can also see that even though there are more expensive properties inside London, more properties are being sold outside London. We can see this on the y-axis where outside London has a higher frequency compared to inside London.

*   Plot that shows the average price per year
"""

data['year'] = data['2001-12-07 00:00'].str[:4]

average = data.groupby('year', as_index=False).mean()
x = average['year']
y = average['52000']

plt.xticks(rotation=90)
plt.xlabel('Year')
plt.ylabel('Price')
plt.title('Average price per year')
plt.plot(x, y)

"""Part 2: Histogram and quantile

# Part 2: Histogram and quantile

Import necessary libraries for this part. Not necessary to get them all but anyways.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

"""Importing data"""

df = pd.read_csv("titanic_train.csv")
df = df.loc[:,["Embarked", "Pclass", "Parch", "Fare"]]

"""## a) Identify the data types of these four columns and plot their distributions

### 1) Identify the data types

*  *Embarked*: The embarkation Embarked column represents a categorical data type. More accurately it is nominal categorical data. As the port of embarkation is a label and there does not exist any order between the different points.

*  *PClass*: Similarly, with the port of embarkation, this data is also categorical. However, as there exists an inherent order between classes this column consists of Ordinal categorical data. 

*  *Parch*: In contrast with earlier explained data, the data 
contained within this column represent numerical data. The main distinction from earlier data is that arithmetic operation such as mean and standard deviation makes logical sense regarding the data. Moreover, the numerical data is finite and countable. Therefore, the data within the column represents numerical countable data. 

*  *Fare*: Simiarly to PClass the data contained within the column Fare is numerical. However, in contrary to Parch the data is contenuous represent real number. Therefore, the data could be classified as numerical continuous data

### 2) plot their distributions
"""

fig, axs = plt.subplots(2, 2)


data = df["Embarked"].value_counts()
axs[0,0].pie(data.values, labels=data.index, autopct='%1.1f%%')
axs[0,0].set_title("Embarked")

data = df["Pclass"].value_counts()
axs[0,1].bar(data.index,data.values)
axs[0,1].set_title("Pclass")
axs[0,1].set(xlabel='class', ylabel='Frequency')

data = df["Parch"].value_counts()
axs[1,0].hist(df["Parch"], bins=10)
axs[1,0].set_title("Parch")
axs[1,0].set(xlabel='Number of children', ylabel='Frequency')

data = df["Fare"].value_counts()
axs[1,1].hist(df["Fare"], bins=20)
axs[1,1].set_title("Fare")
axs[1,1].set(xlabel='price', ylabel='Frequency')
plt.tight_layout()
plt.show()

"""To show the different distribution we have to consider the data type of each container. The first row above (*Embarked* and *Pclass*) represents categorical data types. Hence the barplot and pie charts are suitable. The difference in chart types between these two variables is because *Embarked* is not ordered, in contrast to *Pclass*. The second row represents numerical data and therefore, histograms are a suitable chart to visualize the distribution.

## b)  Histogram
"""

def calcBins(data, bins) ->list:
    min_value = np.min(data)
    max_value = np.max(data)
    delta = (max_value-min_value)/bins
    return ([min_value + delta*i for i in range(0,bins)],delta)

"""The function above *calcBins* is a function that calculates the endpoints of each bin. The intervals between each bin are equally sized and are calculated by dividing the range by the number of bins.

"""

def plotHist(data, bins, title) -> None :
    data_frequency = Counter(data)
    intervals, width = calcBins(data, bins)
    bins = [0 for i in range(0,len(intervals))]
    for value in data_frequency:
        for idx,interval in enumerate(intervals):
            if( value < interval):
                bins[idx] = bins[idx] + data_frequency[value]
                break
    plt.bar(intervals, bins,width=width)
    print("-"*20+title+20*"-") 
    print(list(zip(intervals,bins)))
    plt.show()

"""The function above *plotHist* utilizes the *Counter* function, and the previously defined function *calcBins* to plot a histogram. *Counter* function returns the frequency of each unique value in the dataset. It iterates through this data and adds each frequency to the corresponding bin by utilizing the intervals calculated by *calcBins*. Once each frequency is assigned to its corresponding bin the function utilizes the bar chart where each bin represents a bar. Lastly, it prints out the frequency and the corresponding interval of each data point for the viewer to get a more precise understanding of the chart. 

"""

plotHist(df['Fare'], 20, "Fare")
plotHist(df["Parch"], 10, "Parch")

"""The two histograms above are constructed by utilizing the *plotHist* function. As one can clearly see these charts are nearly identical to those produced by the built-in histogram functions used earlier on the same data.

## c) Data quantile
"""

def calc_quantile(data,q) -> float:
    data = np.sort(data)
    idx = q*(len(data)-1) 
    return data[int(idx)] + (idx % 1) * (data[int(idx)+1]-data[int(idx)])

for i in range(99):
    assert(round(calc_quantile(df["Fare"],i/100),4) == round(np.quantile(df["Fare"],i/100, interpolation="linear"),4))
    assert(round(calc_quantile(df["Parch"],i/100),4) == round(np.quantile(df["Parch"],i/100, interpolation="linear"),4))

"""The function above: *calc_quantile* represents a function that utilizes linear interpolation to calculate the percentiles of a set of data. In order to ensure sufficient precision, an assertion is formalized. The assertion checks that similar values are produced from *calc_quantile* and *quantile*. It should be noted that the output is rounded, however, the deviation of the two outputs is in the magnitude of e7. Therefore, we feel comfortable that the precision of our function is sufficient enough. """

print(calc_quantile(df["Fare"],0.2))
print(calc_quantile(df["Fare"],0.5))

"""Above we utilize the function *calc_quantile()* to calculate percentile 0.2 and 0.5 of the variable "Fare"

# Part 3: Generating data from probabilistic models

This section covers the last question regarding distributions and generation of numbers from these. The implementations will be presented under each subsection, whereafter the discussion about their correspondance with real life data will be held in a final subsection.

Start by importing the few libraries that we will need for the upcoming problem.
"""

import matplotlib.pyplot as plt
from numpy import random

"""## a) Consider the random number generation functions in NumPy"""

# Generate a list with three ndarrays of random values sampled form a uniform distribution 
# with of sizes 100, 1000, and 100000. The values all lies in the intervalu [0,1).
data = [random.rand(100), random.rand(1000), random.rand(100000)]

# Plot the figures next to one another, in order to see changes as n increases.
fig, a = plt.subplots(1, 3, figsize=(7,7), sharex=True)
fig.suptitle('Histograms with data sampled from uniform distribution, with n = 100, 1000, 100000')
a = a.ravel()
for idx, ax in enumerate(a):
    ax.hist(data[idx])

# Now, generate a list with three ndarrays of random values sampled form a normal (Gaussian) distribution 
# with of sizes 100, 1000, and 100000. In this particular case, the parameters mu and sigma (loc and scale)
# have been set to 0 and 2.  
data = [random.normal(0, 2, size=10), random.normal(0, 2, 50), random.normal(0, 2, 10000)]

# Plot the figures next to one another, in order ot see changes as n increases.
fig, a = plt.subplots(1, 3, figsize=(7,7), sharex=True)
fig.suptitle('Histograms with data sampled from normal distribution, with n = 100, 1000, 100000, mu = 0 and sigma = 2', y=1.05)
a = a.ravel()
for idx, ax in enumerate(a):
    ax.hist(data[idx])

plt.tight_layout()
plt.show()

"""**Note on the histograms:**

As we can see from both subplots, with a small number of values being sampled from each respective distribution it is difficult to state from which underlying distribution the values actually have been sampled. For example, you could not really tell by looking at the leftmost graph of the second row that the distribution the values are being sampled from is Gaussian.
This is because a small number of values typically cannot represent a full distribution. The behavior could be represented by a scenario we all know of - the toss of a coin. Sometimes you get many tails in a row and might suspect that there is something wrong witht he coin since you would expect to get a 50/50 division between the heads and tails. A more likely explanation is that you have sampled too few values for them to be representative for the disribution as a whole. Similarly, from the graphs above we can note an overrepresentation of certain values when sample size is "small" - the variations between values are at least higher than what we would expect for a perfect Gaussian or uniform distribution.

However, as we grow the number of values being sampled from the distribution it becomes clear which distributions they are sampled from since they start to resemble the graphs we are used to see from the lectures. The top graphs starts to be filled uniformly over its interval [0, 1) without following the same patterns as the leftmost graph shows. Likewise, the Gaussian starts to take its bell-shaped form.

## b) Modeling a student at an exam
"""

# Function that returns the result of a question, that is the value True if the student managed to answer it right and False otherwise. 
# Thus, the function simulates a Bernoulli process with the probability of success given by the parameter p_success.
# In this function, we recognize that a Bernoulli process is a special case of the binomial distribution, with n=1 and p_succes as given.
def success(p_success):
    result = random.binomial(1, p_success)
    if result == 1:
        return True
    else:
        return False

# Function that loops the previous function n_instances number of times in order to simulate how many correct answers the student manages to answer
# right on an examination. All questions are assumed to be equally likely to be answered right by the student, and this is done with p_correct. 
# Thus, the method simulates a binomial distribution with the aforementioned parameters.
def exam_score(p_correct, n_instances):
    score = []
    for i in range(0, n_instances):
        score.append(success(p_correct))
    return score.count(True)

# Test run with parameters in accordance with assingment.
exam_data = []
for e in range(0, 10000):
    exam_data.append(exam_score(0.8, 20))

# Make pretty histogram.
plt.hist(exam_data, bins=range(0, 21, 1))
plt.suptitle('Histogram over the distribution of exam score and occurrences when 10000 exams have been written')
plt.xlabel('Exam score')
plt.ylabel('Frequency')
plt.show()

"""As we can see from the histogram above, in this run with 10,000 exams being taken - the distributions starts somewhere around the 8 mark. Below that, the probability to score each question is too high to actually generate any **discrete** occurrences. It seems as if the highest frequency is to be found around the 16 points mark.

## c) The persistent student
"""

# Simple function that counts the number of trials needed before the exam was passed. Since the success() function can be used for either a 
# single questions or for exams with assumed equivalence difficulty and format (i.e. number of questions, n, is the same) we will simply
# count the number of times before we return a pass.
# We can do this since all of the attempts are independent and identically distributed (iid) from the success function.
def number_of_attempts(p_pass):
    count = 0
    while not success(p_pass):
        count += 1
    return count

# Run this function 10000 with the probability 0.4. We do this in order to collect the number of sittings needed to pass the exam, 
# when (presumably) having studied just enough to pass it (or at least the equivalency of passing it with p = 0.4).
res = []
for e in range(0, 10000):
    res.append(number_of_attempts(0.4))

# Plot the geometrical distribution over a approriate number of bins so that it looks nice. 
plt.hist(res, bins=range(0,15,1))
plt.suptitle('Histogram over the distribution of the number of attempts that was needed for each exam')
plt.xlabel('Number of trials before passed')
plt.ylabel('Frequency')
plt.show()

"""## General Discussion and Reflection

Firstly, we should note that all of the scenarios above are based on theoretical processes and distributions - thus relying heavily on assumptions that will not be perfectly correspondant to their real-life counterparts in the exam/student situations. As Yinan made clear during the last two lectures, this means that they virtually always will deviate from the results and scenarios that they are aimed at modelling. 

In the case of the particular exam question, you *might* be able to argue that it would be possible to construct questions that are identically difficult for any student to answer, which would correspond to a 40% success rate. However, the student-side of this equation would still be very malleable by the willingness to study of the student. In effect, it would be impossible to construct the situation in which both examination and student knowledge were kept constant, and even more impossible (if there is such a thing as *even more impossible*) to recreate this scenario across multiple (10k) iterations of examinations. If it is the same student taking the exam over and over again, as in the last scenario, it seems highly unlikely that he/she/zher would not gain any knowledge that could affect the probability of success in the next examination. Therefore, it seems, to say the least, highly unlikely that the assumption of iid in the processes would hold true in a real-life situation. 

In short, very much relies on keeping the theoretical process iid in order for these perfect distributions to appear. A consequence of this non-existent independence between test iterations (especially in scenario III), it should be highlighted that it is impossible to keep the parameter p_success/p_pass constant during multiple rounds of iterations in real life. Since this is needed for these perfect, theoretical models to arise from the processes above, we could not expect the result to look like this in reality. This is true both if it is the same student taking the examination multiple times, or if there are different students taking the exam. 

A last note is that these scenarios rely on the assumption that there only exist U/G. While this is true for certain courses at GU, it is certainly not true for the course we are taking right now. Thus, it is not generalizable over very many areas of application.
"""
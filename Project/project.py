# -*- coding: utf-8 -*-
"""Preliminary - Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16lQIhB7X_hPapzCS03gS_HvzdAllMZaY

# Introduction

This report has been written by Group 4 consisting of Evelina Strauss, Jonas Meddeb and Lucas Andrén.

The stucture of this notebook is consistent with the headings of the assignment, however, additional sections have been added to increase the explanatory power.

## Background Scenario and Purpose of the Project

##### **Scenario and Background Setting**
The setting of this investigation takes place in our hypothetical startup, Three Ducks Data Analytics AB, which is specialized in risk management in the financial sector. The Three Ducks has recevied an offer for the development of a model that predicts which customers that should receive a loan - and which that should not.

In the light of this background, the purpose of this particular project is to investigate one possible solution for banks to manage their credit risk. In concrete terms this means that banks only want to provide loans to people who do not risk of defaulting on them. Now, trivial as it sounds, this poses some challenges on the banks related to the assessment of which customers that are more or less likely to default on their loans. Rather than guessing which applicants should receive a loan, there exist a need for more data-driven decision making in this process. 

For the development and evaluation of such a predictive model, we have found a data set containing data on old credit applications, containing a label of whether the applicants have received a bad or good credit risk. This data set could be utilized to construct the predictive model and thereby deliver a  way for the banks to better manage their credit risk/return ratio. The real-life value of this model would presumably arise from a reduced cost structure by the automation of the decision-making process. Secondly, dependent on the robustness and accuracy of the model, it would decrease the banks' risk when issuing credits as the model would ensure that credits are only issued to the customers with little risk of defaulting on them. 

##### **Purpose and Goal of the Project** 
In order to deliver this value, we intend to develop a model which determines if the applicant should receive a loan or not, thus providing the banks with a more robust approach to decision-making. The model will classify each applicant's credit risk as either good or bad, however, there exist some important details to consider in this industry that needs to be taken into consideration. Firsly, true positives (applicants classified as "good" correctly) does not have an equally large positive impact as a misclassification of the false positive type (applicants classified as "good" when in reality they should be classified as "bad"). In fact, a pre-study has shown that the default of one customer's loan costs the bank approximately five times as much as they profit from a successfully repaid loan, on average. 

This will affect the metrics and evaluation process that we will use when evaluating the model, since the decision-making models will need to ensure that the banks reduce their risk of customers' defauling on their credtis. These metrics will be discussed more in-depth later on, and are necessary to quantativly rate the models, since several diffent models will be constructed and compared. The models that we will build are of the following types: 

* Logistic Regression 
* Naive Bayes 
* Random Forests

# Data Specification

The data that we work with consist of customer information from a large bank  in Germany. Each observation of the dataset represent an earlier customer who has previously applied for a loan from the bank. In total, the dataset consist of one thousand observations and 20 various features.

## Imports, Loading and Preparation of the Data

These are the imports that are used throughout this project. All of them have been gathered here in the beginning before the description to not ruin the flow of the report.
"""

# Utils
import random
import math

# Visualization tools
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic

# Data containers
import pandas as pd
import numpy as np

# Scikit-learn
from sklearn import preprocessing
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

# Scipy
import scipy.stats as stats    
from scipy.special import polygamma
from scipy.special import digamma

"""## Source of the Data

The original source of this dataset is from Professor Hofmann at the Universit of Hamburg and could be retrieved from UCI Machine Learning Repository (n.d). 

The information regarding the origin of this data is scarce as the majority of of it is written in German. Therefore, we have had to rely on secondary sources such as Grömping (n.d) report to gather information pertaining the data collection. The data is collected from a large regional bank in southern Germany that has both rural and urban customers. Moreover, the data is heavely oversampled, where 30% of the data represents bad issued credits. In reality, only 5% of the credits issued are classified as "bad". A loan that is classified as "bad" implies that the customer who has applied for the credit has not passed each condition stipulated by the contract.



**Sources**
> UCI Machine Learning Repository: Statlog (German Credit Data) Data Set. (n.d.). Http://Archive.Ics.Uci.Edu. Retrieved 01-12-20, from https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29


> Grömping, U. G. (2019, April). South German Credit Data: Correcting a Widely Used Data Set. Beuth University of Applied Sciences Berlin. http://www1.beuth-hochschule.de/FB_II/reports/Report-2019-004.pdf

## Describe the Data: Types and Features

As previously noted, the dataset consist of one thousand observation and 20 various features that are both categorical and numerical. Below follows a  short description of each feature.The description of each feature is attained by using the description from UCI Machine Learning Repository (n.d). To attian the numeric values of each feature we have manually replaces each string with its coresponding numeric value. To determine what number each string should be converted to we have utelized the documentation available from UCI Machine Learning Repository (n.d).

##### **Input** 
* ***Account Balance*** [Categorical - Ordinal]
 > Status of checking account
 * 1: Less then 0 DM 
 * 2: Between zero and 200 DM 
 * 3: Over 200 DM
 * 4: No checking account. 

* ***Duration of Credit***  [Number - Continuous]
> Duratorn of the loan in months:


* ***Payment Status of Previous Credit*** [Categorical - Nominal] 
> Credit history
  * 0: No credits taken
  * 1: All credits payed back
  * 2: Existing credits paid back 
  * 3: Delay in paying off in the past
  * 4: Other credits existing at other banks


* ***Purpose*** [Categorical - Nominal] 
> Purpose of the loan.
  *0 : car (new)
  *1 : car (used)
  *2 : furniture/equipment
  *3 : radio/television
  *4 : domestic appliances
  *5 : repairs
  *6 : education
  *7 : vacation 
  *8 : retraining
  *9 : business
  *10 : others


* ***Credit Amount*** [Numerical - Continuous]
> Amount of Credit in Dm.

* ***Value Savings/Stocks*** [Categorical - Ordinal]
> Status of saving account.
 * 1: Less than 100 DM
 * 2: Between 100 and 500 DM
 * 3: Between 500 and 1000 DM
 * 4: 1000 DM or more
 * 5: Unknown/no saving


* ***Length of current employment*** [Categorical - Ordinal]
> The total time the customers has been employd at his current job, in years.
 * 1: Unemployed.
 * 2: Below one year 
 * 3: Between one and four years
 * 3: Between four and seven years
 * 4: Above seven ears

* ***Instalment per cent*** [Numerical - Continous]
> Installment rate in percentage of disposable income

* ***Sex & Marital Status*** [Categorical - Nominal] 
> Gender and marital status of the customers.
 * 1: Male divorced/separated
 * 2: Female divorced/separated/married
 * 3: Male single
 * 4: Male married/widowed
 * 5: female single

* ***Guarantors*** [Categorical - Nominal] 
> If people have vouched for the person taking the loan.
  * 1: None
  * 2: Co-applicant
  * 2: guarantor

* ***Duration in current address*** [Numerical - Dsicrete] 
> How long they have lived at their current home adress.

* ***Most valuable available asset*** [Categorical - Nominal] 
 * 1: Real estate
 * 2: Life insurance
 * 3: Car
 * 4: Unknown/None

* ***Age*** [Numerical - Discrete]
> Age of the customer in years.

* ***Concurrent Credits*** [Categorical - Nominal]
>  Credits the customer have concurrently. 
 * 1: Other Banks 
 * 2: Other dept Stores
 * 3: None

* ***Type of apartment*** [Categorical - Nominal]
>  Ownership type of the apartment. 
  * 1: Rent 
  * 2: own
  * 3: for free 

* ***No of Credits at this Bank*** [Numerical - Discrete]
> Number of credits the customer has 
related to this bank 


* ***Occupation*** [Categorical - Ordinal]
> Type of occupation.
 * 1: Unemployed Unskilled
 * 2: Unskilled permanent resident  
 * 3: Skilled 
 * 4: Executive 


* ***No of dependents*** [Numerical - Discrete]
> Number of people being liable to provide maintenance for


* ***Telephone*** [Categorical - Nominal]
> If the customer has a phone.
 * 1: None 
 * 2: Yes 


* ***Foreign Worker*** [Categorical - Nominal]
> Weather or not the customer is a foreign worker
 * 1: Yes
 * 2: No


##### **Target Variable** 
* ***Credibility*** [Categorical - Nominal]
> Credit worthyness
  * 1: Good credit
  * 2: Bad credit

## Reading, Converting and Splitting the Data

### Read and Convert Data

We read the data set from a csv file. Thereafter, we convert some of the data points in order to better handle the data set. The original data set included codings which were difficult to understand and work with, which is why the following replacements are done in accordance with the description above. Converting categorical data to numerical values is necessary, as several of the models that we will utilize requires numerical values to function.
"""

#Reading the original file that was included in the .zip file
DF_Original = pd.read_csv("German_Credit_Data_text.csv", delimiter=';', header=None)

# Replacing string lables with numbers as defined above.
DF_Original.iloc[:,0] = DF_Original.replace({'A11': 1, 'A12' : 2, 'A13' : 3, 'A14' :4})

DF_Original.iloc[:,2] = DF_Original.iloc[:,2].replace({'A30': 0, 'A31' : 1, 'A32' : 2, 'A33' : 3, 'A34' : 4})
DF_Original.iloc[:,3] = DF_Original.iloc[:,3].replace({'A40': 0, 'A41' : 1, 'A42' : 2,
                           'A43' :3, 'A44' :4, 'A45' : 5, 
                           'A46' : 6, 'A47' : 7, 'A48' : 8, 
                           'A49' : 9, 'A410' : 10})
DF_Original.iloc[:,5] = DF_Original.iloc[:,5].replace({'A61': 1, 'A62' : 2, 'A63' : 3, 'A64' :4, 'A65' :5})
DF_Original.iloc[:,6] = DF_Original.iloc[:,6].replace({'A71': 1, 'A72' : 2, 'A73' : 3, 'A74' :4, 'A75' :5})
DF_Original.iloc[:,8] = DF_Original.iloc[:,8].replace({'A91': 1, 'A92' : 2, 'A93' : 3, 'A94' :4, 'A95' :5})
DF_Original.iloc[:,9] = DF_Original.iloc[:,9].replace({'A101': 1, 'A102' : 2, 'A103' : 3})
DF_Original.iloc[:,11] = DF_Original.iloc[:,11].replace({'A121': 1, 'A122' : 2, 'A123' : 3, 'A124': 4})
DF_Original.iloc[:,13] = DF_Original.iloc[:,13].replace({'A141': 1, 'A142' : 2, 'A143' : 3})
DF_Original.iloc[:,14] = DF_Original.iloc[:,14].replace({'A151': 1, 'A152' : 2, 'A153' : 3})
DF_Original.iloc[:,16] = DF_Original.iloc[:,16].replace({'A171': 1, 'A172' : 2, 'A173' : 3, 'A174' : 4})
DF_Original.iloc[:,18] = DF_Original.iloc[:,18].replace({'A191': 1, 'A192' : 2})
DF_Original.iloc[:,19] = DF_Original.iloc[:,19].replace({'A201': 1, 'A202' : 2})
DF_Original.astype("float")
DF_Original.columns = ["Account Balance", "Duration of Credit", "Payment Status of Previous Credit", "Purpose", "Credit Amount",
              "Value Savings/Stocks", "Length of current employment", "Instalment per cent", "Sex & Marital Status",
              "Guarantors", "Duration in Current address", "Most valuable available asset", "Age", "Concurrent Credits",
              "Type of apartment", "No of Credits at this Bank", "Occupation", "No of dependents", "Telephone", "Foreign Worker", "Credibility"]
DF_Original.to_csv('out.csv')

"""### Split the Data

We split the data into two different sets, in order to not show the full data set in the training phase to the models we are going to construct.
"""

def split(X,Y, p):
  # This method shuffles the data and devides it to the following sets: 
  # X_train: feature set for training 
  # Y_train: set of target varaibles associated with the corresponding feature set for training.
  # X_test: feature set for testing.
  # Y_test: set of target varaibles associated with the corresponding feature set for testing.

  # Combining lables and features so we can shuffle the two subsets combined as one set. 
  data = pd.concat([X, Y], axis=1)
  data  = data.sample(frac=1,replace=False).reset_index(drop=True)

  # Find the split point based on parameter p, and constructs the subsets defined above. 
  idx_split = int(data.shape[0] * p)
  data_train = data.iloc[:idx_split,:]s
  data_test = data.iloc[idx_split::,:].reset_index(drop=True)
  return data_train.iloc[:,:-1], data_train.iloc[:,-1], data_test.iloc[:,:-1], data_test.iloc[:,-1]

# Splitting the data that we loaded in previous cell 
X = DF_Original.iloc[:,0:-1]
Y = DF_Original.iloc[:,-1]
X_train, Y_train, X_test, Y_test = split(X,Y,0.8)

"""### Evaluate the Distribution of the Split

In order to determine if the train and test sets follow the same distributions, we construct a QQ-plot to visually discover any discrepancies.
"""

# Method that returns the quantiles for two distributions of data 
def qq_plot(dist_1, dist_2):
  qq_dist_1 = [np.quantile(dist_1, p/100) for p in range(0, 100)]
  qq_dist_2 = [np.quantile(dist_2, p/100) for p in range(0, 100)]
  return qq_dist_1, qq_dist_2

# Construct plots
for idx, data in enumerate([("Duration of Credit (month)",1),("Instalment per cent",4),("Age",12)],1):
  plt.subplot(2,2,idx)
  qq_train, qq_test = qq_plot( X_train.iloc[:,data[1]], X_test.iloc[:,data[1]] )
  plt.scatter(qq_train,qq_test)

  # Add titles and axis titles 
  plt.title(data[0])
  plt.xlabel('qq-test')
  plt.ylabel('qq-train')

# Display plots
plt.tight_layout()
plt.show()

"""As we can see from all three of the plots, it seems as if the quantiles of the training sets matches the quantiles from the test set. Not only are they linearly correlated, but they also seem to be from the exact same distribution since they seem to show the 45 degree derivative mentioned in class. We can therefore, with some confidence, state that the tests are representative for one another and that no major discrepancies exists after the split. Thus, we deem it safe to save the test set until the end for evaluation purposes.

# Define a Problem

As described in the background of the project in the *Introduction* chapter, there exist problems for banks to manage their credit risks related to providing loans to customers. This is a decision that could be improved by a data-driven approach, which is something that almost self-evidently provides us with underlying rationales for this implementation. However, for the sake of the project the explicit problem formulation and motivations are presented below. 

##### **Problem statement and motivation**
In this project, we will use the *credit risk data set* to build a classifier that classifies which customers should receive a loan or not, in order to help banks better manage their credit risk/return ratio. Different models will be used in order to balance a suitable evaluation metric and the complexity of the model. Metrics and models will be discussed later on in this notebook.

# Descriptive Analysis

## Histograms and bar charts of Selected Variables
"""

# Histograms and bar charts of selected variables

sns.set_style('white') 
fig, ax = plt.subplots(4,3,figsize=(10,10))
sns.histplot(X_train.iloc[:,1], ax=ax[0][0])
sns.histplot(X_train.iloc[:,4], ax=ax[0][1])
sns.histplot(X_train.iloc[:,12], ax=ax[0][2])
sns.countplot(x=X_train.iloc[:,7], ax=ax[1][0], palette=sns.color_palette("Blues")[3:])
sns.countplot(x=X_train.iloc[:,0], ax=ax[1][1], palette = sns.color_palette('Blues')[3:])
sns.countplot(x=X_train.iloc[:,5], ax=ax[1][2], palette = sns.color_palette('Blues'))
sns.countplot(x=X_train.iloc[:,2], ax=ax[2][0], palette = sns.color_palette('PuBu'))
sns.countplot(x=X_train.iloc[:,15], ax=ax[2][1], palette = sns.color_palette('PuBu')[3:])
sns.countplot(x=X_train.iloc[:,8], ax=ax[2][2], palette = sns.color_palette('Blues'))
sns.countplot(x=X_train.iloc[:,17], ax=ax[3][0], palette = sns.color_palette('Paired'))
sns.countplot(x=X_train.iloc[:,14], ax=ax[3][1], palette = sns.color_palette('Paired'))
sns.countplot(x=Y_train, ax=ax[3][2], palette = ['g', 'r'])

ax[0][0].set_xlabel('Duration of credits (months)')
ax[0][1].set_xlabel('Credit amount (DM)')
ax[0][2].set_xlabel('Age')
ax[1][0].set_xlabel('Instalment per cent')
ax[1][1].set_xlabel('Account Balance')
ax[1][2].set_xlabel('Value Savings')
ax[2][0].set_xlabel('Payment status of previous credit')
ax[2][1].set_xlabel('No of credits at this bank')
ax[2][2].set_xlabel('Sex & Martial status')
ax[3][0].set_xlabel('No of dependents')
ax[3][1].set_xlabel('Type of apartment')
ax[3][2].set_xlabel('Credibility/target value')

plt.subplots_adjust(hspace=0.4, wspace=0.3)
plt.tight_layout()
plt.show()

"""**Observations from the histograms and bar charts**

The above histograms and bar charts provide an overview of some key variables in the dataset. We see, for example, that the majority of all loans are between 0-5000 DM, most people have less than 100 DM in value savings and the majority is labeled as good credit. From the histograms, we can conclude that there are skewed distributions for Duration of credits, Credit amount and Age. All three distributions are skewed to the right, meaning that most values lies on the left side of the histogram. 

The bar charts provides information about the distribution between the different categories. There are some categories which are overrepresented, such as category "1" in Value savings, "1" in No of dependent and "2" in Payment status of previous credits. 

We have selected these variables as we believe that they will play a central role in the future classification of new customers and their loans. It is important to understand the dataset on which the classifier is trained to understand which parts play a central role in the final result.

## Dependence of Selected Variables

To investigate the dependency of selected variables, we have chosen to visualize such dependencies with scatterplots and mosaic plots depending on whether the variables are of a categorical- or numerical data type.
"""

# Scatterplots to show dependency of selected variables

sns.set_style('white') 
fig, ax = plt.subplots(2,3,figsize=(10,10), squeeze=False)
sns.scatterplot(x=X_train.iloc[:,1], y=X_train.iloc[:,4], ax=ax[0][0])
sns.scatterplot(x=X_train.iloc[:,15], y=X_train.iloc[:,4], ax=ax[0][1])
sns.scatterplot(x=X_train.iloc[:,12], y=X_train.iloc[:,4], ax=ax[0][2])
sns.scatterplot(x=X_train.iloc[:,1], y=X_train.iloc[:,12], ax=ax[1][0])
sns.scatterplot(x=X_train.iloc[:,5], y=X_train.iloc[:,4], hue=Y_train.values, ax=ax[1][1], palette=['g', 'r'])

ax[0][0].set_xlabel('Duration of credit')
ax[0][0].set_ylabel('Credit amount (DM)')
ax[0][1].set_xlabel('Number of credits at this bank')
ax[0][1].set_ylabel('Credit amount (DM)')
ax[0][2].set_xlabel('Age')
ax[0][2].set_ylabel('Credit amount (DM)')
ax[1][0].set_xlabel('Duration of credit')
ax[1][0].set_ylabel('Age')
ax[1][1].set_xlabel('Value savings')
ax[1][1].set_ylabel('Credit amount (DM)')

plt.tight_layout()
plt.show()

"""**Observations from the scatterplots**

Based on these scatterplots, one can conclude that there exist a relationship between some of the variables. There is a positive relationship between "Duration of credits" and "Credit amount (DM)". Although, there is a low correlation between the two variables as the data is scattered, meaning that the relationship is weak. We can also identify a negative relationship between the "Number of credits at this bank" and "Credit amount (DM)" as the Credit amount decreases as the number of credits at this bank increases. 

Between "Duration of credit" and "Age", we can state that most age groups have a low duration of credits. Although, the data is still a bit scattered, meaning that there is no relationship between "Age" and "Duration of credits". The same goes for the dependency between "Age" and "Credit amount (DM)" as most age groups have a credit amount under 7500 DM. There are higher values for credit amounts for younger people compared to older. But if we look at the previous histogram, we notice that the age distribution is skewed right, meaning that there are more young people in the dataset which explains why there are more points to the left.

The last scatterplot provide more depth as the target value, Credibility, is included. The plot gives us insight into the relationship between "Value savings" and "Credit amount (DM)". We can see that as value saving increases, the credit amount decreases. And as credit amount decreases and value savings increases, the number of good credits increase.
"""

# Mosaic plot to show dependency of some selected variables

fig, ax = plt.subplots(4,3,figsize=(15,15), squeeze=False)

mosaic(X_train, ['Payment Status of Previous Credit', 'Sex & Marital Status'], ax=ax[0][0], gap=0.01)
mosaic(X_train, ['Payment Status of Previous Credit', 'No of Credits at this Bank'], ax=ax[0][1], gap=0.01)
mosaic(X_train, ['Account Balance', 'Value Savings/Stocks'], ax=ax[0][2], gap=0.01)
mosaic(X_train, ['Most valuable available asset', 'Type of apartment'], ax=ax[1][0], gap=0.01)
mosaic(X_train, ['Foreign Worker', 'Account Balance'], ax=ax[1][1], gap=0.01)
mosaic(X_train, ['Occupation', 'Value Savings/Stocks'], ax=ax[1][2], gap=0.01)
mosaic(X_train, ['Foreign Worker', 'Type of apartment'], ax=ax[2][0], gap=0.01)
mosaic(X_train, ['Payment Status of Previous Credit', 'Value Savings/Stocks'], ax=ax[2][1], gap=0.01)
mosaic(X_train, ['Concurrent Credits', 'Account Balance'], ax=ax[2][2], gap=0.01)
mosaic(X_train, ['Value Savings/Stocks', 'Sex & Marital Status'], ax=ax[3][0], gap=0.01)
mosaic(X_train, ['Foreign Worker', 'Value Savings/Stocks'], ax=ax[3][1], gap=0.01)
mosaic(X_train, ['Type of apartment', 'Sex & Marital Status'], ax=ax[3][2], gap=0.01)

labels = [('Payment status of previous credit','Sex & martial status'), ('Payment status of previous credit','No of credits at this bank'),
          ('Account Balance','Value savings'),('Most valuable asset','Type of apartment'),('Foreign worker','Account Balance'),
          ('Occupation','Value Savings'), ('Foreign worker','Type of apartment'),('Payment status of previous credit','Value savings'),
          ('Concurrent credits','Account Balance'), ('Value savings','Sex & martial status'),('Foreign worker','Value savings'),
          ('Type of apartment','Sex & martial status')]

counter = 0
for x_idx in range(0,4):
  for y_idx in range(0,3):
    ax[x_idx][y_idx].set_xlabel(labels[counter][0])
    ax[x_idx][y_idx].set_ylabel(labels[counter][1])
    counter += 1

plt.tight_layout()
plt.show()

"""**Observations from the mosiac plots**

The mosaic plots above gives us an indication of the dependency between some categorical variables in the dataset. The most significant correlations can be found between:

*   Payment status of previous credits and Number of credits at this bank
*   Occupation and Value savings
*   Foreign worker and Type of apartment
*   Payment status of previous credits and Value savings
*   Foreign worker and Value savings
*   Type of apartment and Sex & martial status

Due to some categories being overrepresented, one should be careful about drawing conclusion based on only these mosaic plots. For example, category "1" in "Foreign worker" is clearly overrepresentative, meaning that it will show a correlation between many categories which also are overrepresentative since most data points lies in such categories. But it still gives us an indication of the correlation between some categorical variables.

## Describe the Data

Below are some descriptive statistics of the selectected variables deemed most interesting for this project. From the statistics, we can already here see that the data in some of the variables probably look rather skewed when we visualize them i the next sections.

#### Sample Mean, Sample Standard Deviation and Quantiles of Selected Variables
"""

numerical_values = X_train[['Duration of Credit', 'Credit Amount', 'Age']]
numerical_values.describe()

"""**Duration of credits**
*   Mean: 21.14
*   Std: 12.15
*   Q1: 12.0
*   Q2: 18.0
*   Q3: 24.0

**Credit amount**
*   Mean: 3331.73
*   Std: 2875.36
*   Q1: 1381.75
*   Q2: 2328.0
*   Q3: 4025.5

**Age**
*   Mean: 35.41
*   Std: 11.21
*   Q1: 27.0
*   Q2: 33.0
*   Q3: 42.0

#### Range of Selected Variables
"""

duration_range = X_train['Duration of Credit'].max() - X_train['Duration of Credit'].min()
credit_range = X_train['Credit Amount'].max() - X_train['Credit Amount'].min()
age_range = X_train['Age'].max() - X_train['Age'].min()
data_range = {'Duration of Credit': [duration_range], 'Credit Amount': [credit_range], 'Age': [age_range]}
df = pd.DataFrame(data_range, index=['range'])
print(df)

"""## Visualization to Explore the Data Set"""

# Bar charts to visualize the data set

sns.set_style('white')
fig, ax = plt.subplots(2,3,figsize=(10,10), squeeze=False)
sns.countplot(x=X_train['Account Balance'], hue=Y_train, ax=ax[0][0], palette=['g', 'r'])
sns.countplot(x=X_train['Value Savings/Stocks'], hue=Y_train, ax=ax[0][1], palette=['g', 'r'])
sns.countplot(x=X_train['Sex & Marital Status'], hue=Y_train, ax=ax[0][2], palette=['g', 'r'])
sns.countplot(x=X_train['Payment Status of Previous Credit'], hue=Y_train, ax=ax[1][0], palette=['g', 'r'])
sns.countplot(x=X_train['Type of apartment'], hue=Y_train, ax=ax[1][1], palette=['g', 'r'])
sns.countplot(x=X_train['Length of current employment'], hue=Y_train, ax=ax[1][2], palette=['g', 'r'])

ax[0][0].set_xlabel('Account balance')
ax[0][1].set_xlabel('Value savings')
ax[0][2].set_xlabel('Sex & martial status')
ax[1][0].set_xlabel('Payment status of previous credits')
ax[1][1].set_xlabel('Type of apartment')
ax[1][2].set_xlabel('Length of current employment')

plt.tight_layout()
plt.show()

"""**Observations from bar charts**

The most interesting observation can be found in "Payment of previous credits" where category "0" and "1" have a higher frequency of bad credits. Category "1" and "2" stands for "No credits taken" and "All credits payed back". We would have expected it to be the other way around as those two categories are considered good. We would have expected category "3" to have higher frequency of bad credit as it stands for "Delay in paying off in the past". The explanation could be that there are a number of other variables that have affected the credibility, although that is only a guess and there could be another answer to the question. 

The remaining graphs are in line with what we had expected, for example that there is a greater difference in the number of good and bad credits as "Value savings" increases. 
"""

# Box plots to visualize the data set

sns.set_style('white') 
fig, ax = plt.subplots(3,2,figsize=(15,15), squeeze=False)
sns.boxplot(x=X_train['Value Savings/Stocks'], y=X_train['Credit Amount'], hue=Y_train, ax=ax[0][0], palette=['g', 'r'])
sns.boxplot(x=X_train['Value Savings/Stocks'], y=X_train['Age'], hue=Y_train, ax=ax[0][1], palette=['g', 'r'])
sns.boxplot(x=X_train['Value Savings/Stocks'], y=X_train['Duration of Credit'], hue=Y_train, ax=ax[1][0], palette=['g', 'r'])
sns.boxplot(x=X_train['Sex & Marital Status'], y=X_train['Credit Amount'], hue=Y_train, ax=ax[1][1], palette=['g', 'r'])
sns.boxplot(x=X_train['Account Balance'], y=X_train['Credit Amount'], hue=Y_train, ax=ax[2][0], palette=['g', 'r'])
sns.boxplot(x=X_train['Purpose'], y=X_train['Credit Amount'], hue=Y_train, ax=ax[2][1], palette=['g', 'r'])

labels = [('Value savings','Credit Amount (DM)'),('Value savings','Age'),('Value savings','Duration of credit'),
          ('Sex & Martial status','Credit Amount (DM)'), ('Account Balance','Credit Amount (DM)'),('Purpose','Credit Amount (DM)')]

counter = 0
for x_idx in range(0,3):
  for y_idx in range(0,2):
    ax[x_idx][y_idx].set_xlabel(labels[counter][0])
    ax[x_idx][y_idx].set_ylabel(labels[counter][1])
    counter += 1

plt.tight_layout()
plt.show()

"""**Observations from boxplots**

As we have mentioned before and can be seen from the boxplots above is the fact that some variables are unevenly distributed, resulting in many outliers. It can be seen very clearly between "Value savings" and "Credit amount (DM)" as the first category has many outliers. By identifying such outliers, one can remove them for getting "better" results as outliers may have a negative effect on the final result. The first category in "Value savings" is "Less than 100 DM", so it makes sense that such category would have longer whiskers and more outliers as they want higher credit amount. Whereas category "3" and "4" has shorter whiskers and IQR and therefore apply to less credit amount due to more savings. We can also see that for all categories, bad credit has a higher value of Q3 compared to good credit, meaning that people labeled with bad credit has asked for higher credit amount compared to people labeled with good credit.

The biggest difference between good and bad credit can be found in boxplot with "Purpose" and "Credit amount (DM)". There is a difference both between the categories, but also in credibility. Most other boxplots are quite similar between the different categories, but in purpose we can see a big difference. It is for example clear that category "10" ask for less credit amount compared to bad credit in the same category. Category "10" stands for "others" which makes it hard to draw conclusions on the reason to the big difference. 

These were some examples of the interesting observations from the boxplots, one can find more by observing and investigating the plots.

## Concluding Remarks and Relevance of Selected Variables

Before we start conducting various tests, analyzes and models, it is essential to get an overview of the data and its variables. In the histograms and bar chart, we notice much of the data is skewed and overrepresentive for certain features. Such awareness is central to further analysis and processing of the data. Skewness and overrepresentation affect, for example, a classification model because when we train the model, certain values ​​occur more often than others, which leads to the model performing better on features with higher frequency. Since our job is to create such classification model, it is essential that we have knowledge about skewdness and overepresenative features when we train and evaluate our model. Likewise, any outliers can have a major impact on the final result. By identifying such observations, we can then process the data further and remove outliers to get the best end result possible. It is also important for us to understand the distributions of the variables before future processes where for example, hypothesis testing is to be carried out and we are to choose suitable classification models for the bank. Distributions affect such results and choices, in what way is to be examined further.

When the model is to classify a customer with good- or bad credit, there are a number of different variables that come into play and affect the final result. The dataset is large with many variables and therefore one can not include them all in all analyzes. But after examining and understanding the datset we have chosen suitable visualization methods for most variables. Some variables occur more often that others as we believe they have a greater impact on the final result. We have for example, chosen to include "Value savings" in most analyzes as we believe that this variable has a central role in the classification. For example, we have not included "Telephone" as we do not see the relevance of the variable for the process as we believe most people have a telephone. Whether these assumptions are consistent with reality remains to be seen when the final analysis is completed.

# Probability Distribution

## Distributions of Some Selected Variables

After having thoroughly described and reviewed the data in the last section, we can understand that the distributions that most our continous variables follow not are normal distributions. Instead, they seem to be skewed in certain ways. This seems natural for some of them since there exists limitations to the amount of credit you can have (i.e. we do not measure negative credits – savings). Furthermore, you have to be of a certain age to even receive credit, and it seems if you are more likely to take on credit when you are younger. Such factors affect the distributions seen above. However, we can still try to understand which distributions that the data follows by mapping their quantiles towards some standard distributions. In this section, we will direct particular focus to the two continuous variables, age and credit amount, since they are deemed most interesting from both the perspective of modeling and importance for the upcoming sections.

### Credit Amount Distribution

Let us start with the distribution over the **credit amount**. We can understand that we are looking for some sort of skewed distribution from the graph above. Since we have heard about the gamma distribution (not included in this course) before, we can make an educated guess that it will follow this distribution, and thus compare the data against it. We can plot both the qq-plot of the normal and gamma distributions to see if there are any interesting findings to dig deeper into.
"""

credit_amount = X_train.iloc[:,4]

normal_data = np.random.normal(0, 1, len(credit_amount)) 
gamma_data = np.random.gamma(1, 2, len(credit_amount))
distributions = [("Normal distribution", normal_data), ("Gamma distribution",gamma_data)]

# Start by plotting the data in order to undertand how it looks
plt.hist(credit_amount)
plt.title("Credit amount")
plt.show()

#Constructing plots
for idx, data in enumerate(distributions,1):
  plt.subplot(2,2,idx)
  qq_x, qq_y = qq_plot(credit_amount, data[1])
  plt.scatter(qq_x, qq_y)
  plt.title(data[0])
  # Adding title and axis 
  plt.xlabel('qq-dist-1')
  plt.ylabel('qq-dist-2')

# Display QQ-plots 
plt.tight_layout()
plt.show()

"""As can be seen from the two qq-plots above, it is rather obvious that the normal distribution shows a large non-linear relationship to the data. The initial fit between the gamma distribution, however, seems to be much more explanatory. This is something we furhter will investigate later, since we simply have guessed the parameters of this example. It should be noted that we could, and perhaps should, have included multiple different variants of these distributions - i.e. many distribution with different parameters. However, it is pretty clear that the data fits better to a gamma distribution than a normal distribution, so we will not consider the difference further in this section.

### Age Distribution

Since we do not have that many continous variables to choose from, we make a similar investigation of the age distirbution just to see that we are on the right track in regards to the distributions. 

Again, the same pattern is displayed with a skewed distribution which looks more similar to the gamma above than a normal distribution.
"""

age_data = X_train.iloc[:,12]

normal_data = np.random.normal(0, 1, len(age_data)) 
gamma_data = np.random.gamma(1, 2, len(age_data))

distributions = [("Normal distribution", normal_data), ("Gamma distribution",gamma_data)]

plt.hist(age_data)
plt.title("Age distribution")
plt.show()

#Construct plots
for idx, data in enumerate(distributions,1):
  plt.subplot(2,2,idx)
  qq_x, qq_y = qq_plot(age_data, data[1])
  plt.scatter(qq_x, qq_y)
  plt.title(data[0])
  # Adding title and axis 
  plt.xlabel('qq-dist-1')
  plt.ylabel('qq-dist-2')

# Display QQ-plots
plt.tight_layout()
plt.show()

"""## Parameter Estimation

So, now that we know that we are looking for two gamma distributions we have to contend with the fact that these are outside of the scope of this course. However, we can take the simple way out and employ the built-in methods from scipy to still estimate the parameters of the distributions - even if this means that we will miss out of a lot of the understanding.

For the full estimation version, please refer to the appendix where we have placed our own version of the same code. This was included for learning purposes and we do not suggest that the opponent looks into this, if its not in the interest of this(these) person(s).
"""

# Estimating the age data distribution using the built-in method
x = np.linspace(min(age_data), max(age_data), 10000)
fit_alpha, fit_loc, fit_beta = stats.gamma.fit(age_data)
fit_data = stats.gamma.rvs(fit_alpha, size=10000)

plt.figure(figsize=(12,6))
plt.subplot(121)
# Create pdf from our estimated parameters
gamma_pdf = stats.gamma.pdf(x, fit_alpha, fit_loc, fit_beta)
hist = plt.hist(age_data, density=True, color='green', alpha=0.4)
pdf = plt.plot(x, gamma_pdf, color='orange')

plt.subplot(122)
# Plot the QQ-plots to see how well estimated the parameters are at describing the data
qq_x, qq_y = qq_plot(age_data,fit_data)
plt.scatter(qq_x, qq_y)
plt.title("Quantiles from estimated distribution against age data")
# Adding title and axis 
plt.xlabel('qq-dist-1')
plt.ylabel('qq-dist-2')
plt.show()

print("Estimated shape (alpha) for the distribution: {}\nEstimated rate (beta) for the distribution: {}".format(fit_alpha, fit_beta))

"""In this first example shown above, we utilize the bult-in fit method to estimate the parameters for the age data. It seems as if the fitted gamma distribution is a good choice with the printed parameters. Not only is a 45-degree line shown in the QQ-plot, but we can also visually determine that the distribution fits to the histogram. Therefore, we feel confident that this  selected variable can be modeled with this gamma distribution. 


As mentioned above, we could try to achieve the same estimation without the use of the fit-method. This version has been implemented below, and is explained/derived in the appendix.
"""

# Estimating the credit amount using our own method

# Maximum likelihood estimation method for estimating parameters, see appendix for explanation
def MLE_gamma(x):
  # The tolerance and distance parameters are responsible for the accuracy of the approximation 
  tolerance = 0.00001
  distance = float('inf')
  # Variables needed to compute the new value
  p0 = 1
  n = len(x)
  mean = x.mean()
  log_sum = np.log(x).sum()

 # Iterative steps related to the Gauss-Newton method 
  while distance > tolerance:
    f_prim = n/p0-n*polygamma(1,p0)
    f = n*math.log(p0)-n*math.log(mean) + log_sum-n*digamma(p0)
    p_1 = p0 - f/f_prim
    distance = abs(p_1 - p0)
    p0 = p_1
  return p0, mean/p0

  
# Return parameters and sample from distribution with these parameters
x = np.linspace(min(credit_amount), max(credit_amount), 10000)
estimated_shape, estimated_rate = MLE_gamma(credit_amount)
estimated_data = stats.gamma.rvs(estimated_alpha, size=10000)

plt.figure(figsize=(12,6))
plt.subplot(121)
# Create pdf from our estimated parameters
gamma_pdf = stats.gamma.pdf(x, estimated_shape, scale=estimated_rate)
hist = plt.hist(credit_amount, density=True, color='green', alpha=0.4)
pdf = plt.plot(x, gamma_pdf, color='orange')


# Plot the QQ-plots to see how well estimated the parameters are at describing the data
plt.subplot(122)
qq_x, qq_y = qq_plot(credit_amount, estimated_data)
plt.scatter(qq_x, qq_y)
plt.title("Quantiles from estimated distribution against credit amount data")
# Adding title and axis 
plt.xlabel('qq-dist-1')
plt.ylabel('qq-dist-2')
plt.show()

print("Estimated shape (alpha) for the distribution: {}\nEstimated rate (beta) for the distribution: {}".format(estimated_shape, estimated_rate))

"""It seems as if we have managed to master the estimation process rather well for the gamma distribution using a maximum likelihood estimator as well. Even if the QQ-plot not shows of the perfect 45-degree line that we are after, it still clearly shows the linear relationship at an angle which is satisfactory. It is also much better than our initial guess from the QQ-plots in the "Distributions of Some Selected Variables"-section, even it might be the case that a local optimum has been found rahter than a global one. Our visual inspection further shows that the gamma distribution fits rather well for the histogram plot to the left with the parameters that have been printed out. 

What we can conclude is that the gamma distributions, in accordance with our initial guess from the comparison with the normal distribution, seems to be a possible distribution for modeling the data. Now, it should be mentioned that there exist many other continuous probability distributions, some of which might fit better to these two selected variables. These should all be tested for in the explorative phase above (not limiting to normal distribution and gamma distribution, as we did). However, we chose to focus on the gamma distribution as an educated guess, and only on the two continuous distributions, since we deemed it enough given that these skewed distributions not are included in the course. We also found it more interesting, and  challenging, to understand the methodology behind parameter estimation for such distributions - although it is not included in the course.

## Hypothesis Testing

If we want to show something interesting from the data using hypothesis testing, we can start by considering the following graphs.
"""

# Add the labels to the end of the X_train data set
data = pd.concat([X_train, Y_train], axis=1)

# Split the credit amounts based on if they have received a good or bad credit risk label
good_age = data.loc[lambda x: x.iloc[:,20]==1].iloc[:,12]
bad_age = data.loc[lambda x: x.iloc[:,20]==2].iloc[:,12]
good_age.reset_index(drop=True, inplace=True)
bad_age.reset_index(drop=True, inplace=True)

plt.figure(figsize=(10,10))
plt.subplot(2,2,1)
plt.hist(good_age, alpha=0.4, color='orange', label='Good credit')
plt.hist(bad_age, alpha=0.4,color='green', label='Bad credit')
plt.legend(loc='upper right')
plt.title('Age split on good and bad credibility')

# Split the credit amounts based on if they have received a good or bad credit risk label
good_credit_amount = data.loc[lambda x: x.iloc[:,20]==1].iloc[:,4]
bad_credit_amount = data.loc[lambda x: x.iloc[:,20]==2].iloc[:,4]
good_credit_amount.reset_index(drop=True, inplace=True)
bad_credit_amount.reset_index(drop=True, inplace=True)

plt.subplot(2,2,2)
plt.hist(good_credit_amount, alpha=0.4, color='orange', label='Good credit')
plt.hist(bad_credit_amount, alpha=0.4,color='green', label='Bad credit')
plt.legend(loc='upper right')
plt.title('Credit amount split on good and bad credibility')

plt.show()

"""These graphs are based on the variables *Credit amount* and *Age* but split between those who previously have received a good credit risk score and those who received a bad one. From this split, it is difficult to visually determine if there is a difference in both ages and credit amounts between the ones who receive a bad risk and the other group. We can determine if this is the case by conducting a hypothesis test.

### Hypothesis Testing on Age

So, let us construct a hypothesis test that determines if there can exist an effect on a person's potential of receiving a bad or good credit risk based on their age. That is, are there any differences in the age of people who reveive a "bad" credit risk versus the ones who receive a "good" one. Since we do not know anything about the underlying population, including the standard deviation, we will conduct a t-test. Furthermore, we can treat the old and young people as two independent samples: resulting in a two-sampled t-test. This could look something like the following, structured in the same way as in lectures for clarity:

---


> **Statement:** The average age for receiving a good credit risk is the same as the one of receiving a bad credit risk.

>  **Experiment:** We collect the ages from 100 people from the sample containing people who have received a bad credit risk $N_{BAD}=100$. Thereafter, we collect the ages from 100 people from the other sample, with good credit risk $N_{GOOD}=100$.

>  **Data:**
>>  $bad_{1} ... bad_{N_{BAD}}$ ages of people receiving a bad credit risk; random variable $BAD_{1} ... BAD_{N_{BAD}}$ which is i.i.d.

>> $good_{1} ... good_{N_{GOOD}}$ ages of people receiving a bad credit risk; random variable $GOOD_{1} ... GOOD_{N_{GOOD}}$ which is i.i.d.

>> $BAD_{i}$ and $GOOD_{j}$ independent, for $i=1, ... , N_{BAD}, j=1 ... , N_{GOOD}$

> **Parameter of interest:** 
>>Mean value of age for people receiving bad credit $\mu_{bad}$; We use the estimate $\hat\mu_{bad}=\bar{bad}= \frac{1}{N_{BAD}}\sum_{i=1}^{N_{BAD}}bad_{i}$

>>Mean value of age for people receiving god credit $\mu_{good}$; We use the estimate $\hat\mu_{good}=\bar{good}= \frac{1}{N_{GOOD}}\sum_{i=1}^{N_{GOOD}}good_{i}$

> **Null hypothesis H0:**  $H_{0}: \mu_{bad} = \mu_{good}$

> **Test statistic:**  $t_{0} = \frac{\bar{bad}-\bar{good}}{\sqrt{\frac{s_{BAD}^{2}}{N_{BAD}}+\frac{s_{GOOD}^{2}}{N_{GOOD}}}}$


> **Null distribution:** Student-t distribution with degree of freedom $df=\frac{\left (\frac{s_{BAD}^{2}}{N_{BAD}}+\frac{s_{GOOD}^{2}}{N_{GOOD}}  \right )^{2}}{\frac{\left (\frac{s_{BAD}^{2}}{N_{BAD}}  \right )^{2}}{\left (N_{BAD}-1  \right )}+\frac{\left (\frac{s_{GOOD}^{2}}{N_{GOOD}}  \right )^{2}}{\left (N_{GOOD}-1  \right )}}$ where the sample standard deviations are:

>$s_{X}=\sqrt{\frac{1}{\left(N_{BAD}-1\right)}\sum_{i=1}^{N_{BAD}}\left ( bad_{i}-\bar{bad} \right )^{2}}$
$s_{GOOD}=\sqrt{\frac{1}{\left(N_{GOOD}-1\right)}\sum_{i=1}^{N_{GOOD}}\left ( good_{i}-\bar{good} \right )^{2}}$

> **Alternative hypothesis HA:** $H_{A}: \mu_{bad} ≠ \mu_{good}$ which results in a two-tailed test

> **Significance level $\alpha$:** 0.05
"""

# Sample out 100 random people's ages from the "bad" group
rand_idx = [random.randint(0,len(bad_age)-1) for x in range(0, 100)]
bad_age_sample = [bad_age[idx] for idx in rand_idx]

# Sample out 100 random people's ages from the "good" group
rand_idx = [random.randint(0,len(good_age)-1) for x in range(0, 100)]
good_age_sample = [good_age[idx] for idx in rand_idx]

"""We could carry out this test by manually by calculating all the relevant statistics and values as has been done below for learning purposes.  """

# Collect size of the two samples for shorter methods
n_bad = len(bad_age_sample)
n_good = len(good_age_sample)

# Calculate the mean value for the two samples
mean_bad = np.mean(bad_age_sample)
mean_good = np.mean(good_age_sample)

# Calculate the sample standard deviations for the two samples
sstd_bad = np.std(bad_age_sample)
sstd_good = np.std(good_age_sample)

# Calculate test statistic
t_statistic = (mean_bad - mean_good) / np.sqrt(sstd_bad**2/n_bad + sstd_good**2/n_good)

# Calculate degrees of freedom for null distribution
deg_free = ((sstd_bad**2/n_bad + sstd_good**2/n_good)**2) / ((((sstd_bad**2/n_bad)**2)/(n_bad-1))+(((sstd_good**2/n_good)**2)/(n_good-1)))

# Calculate the p-value
p_value = (1.0 - stats.t.cdf(q, deg_free)) * 2

print("t-stat:{:.3f}\np-value:{:.3f}".format(abs(t_statistic), p_value))

"""Or simply use the scipy library stats to retrieve an almost identical result through the rows below. """

t_stat, p_val = stats.ttest_ind(good_age_sample, bad_age_sample, equal_var=False)
print("t-stat: {:.3f}\np-value: {:.3f}".format(abs(t_stat), p_val))

"""When we evaluate this test, we can clearly see that the p-value is larger than the significance level $\alpha=0.05$ that we choose before carrying out the experiment and test. Therefore, we fail to reject the null hypothesis and cannot state that the ages between the two samples differ nor not differ. This is interesting information to carry on to our classification model, since it possibly can suggest that our age variable not really will have a very large effect on the classification of a "good" or "bad" credibility. Thus, this could constitute a thread to further investigation in the classification-step if deemed interesting.

### Hypothesis Testing on Credit Amount

We could carry out a similar test for the credit amount variable, in order to determine whether there exist any statistical significance in previous credit amount between people who receive a "bad" and "good" credibility. Since we will carry out a test which structure-wise is almost identical to the one above, we have not pasted the framework here. Instead, please refer to the last test with the variable age switched out to the credit amount. Furthermore, in this test we will simply carry out the shorter version utilizing the built-in functions of the independent t-test.
"""

# Sample out 100 random people's credit amount from the "bad" group
rand_idx = [random.randint(0,len(bad_credit_amount)-1) for x in range(0, 100)]
bad_credit_sample = [bad_credit_amount[idx] for idx in rand_idx]

# Sample out 100 random people's credit amount from the "good" group
rand_idx = [random.randint(0,len(good_credit_amount)-1) for x in range(0, 100)]
good_credit_sample = [good_credit_amount[idx] for idx in rand_idx]

# Calculate and print t-statistics and p-value
t_stat, p_val = stats.ttest_ind(good_credit_sample, bad_credit_sample)
print("t-stat: {:.3f}\np-value: {:.3f}".format(abs(t_stat), p_val))

"""This time, the hypothesis test returns a p-value that is smaller than the chosen $\alpha$ of 0.05. This means that we have found a value for our test statistic that is so far out on one tail that it is found in the darkness of the rejection zone. From a statistical point of view, this means that we can reject our null hypothesis, thus accepting the alternative hypothesis. Qualitatively, this means that there seems to exist a difference in the credit amount of the people who have received a "bad" credibility and those who have received a "good" one.

This is something that we can remember since it might suggest that this variable will have a larger impact on the classification process of our model, and that we therefore not should eliminate it in order to seek simplification of the classifier. However, since we only have showed this for two variables out of the 20 we have in our data set, we cannot draw too large conclusions yet. Nevertheless, it is something that is interesting from a hypothesis testing point of view.

# Predictive Analysis

## Definition of Models

### Logistic Regression
"""

def add_dummies(X):
  columns_dict = {"Account Balance" : [1,2,3,4],
             "Duration of Credit" : [''],
             "Payment Status of Previous Credit" : [0,1,2,3,4],
             "Purpose" : [0,1,2,3,4,5,6,7,8,9,10],
             "Credit Amount" : [""],
             "Value Savings/Stocks" : [''],
             "Length of current employment" : [1,2,3,4,5], 
             "Instalment per cent" : [""],
             "Sex & Marital Status": [1,2,3,4,5],
             "Guarantors" : [1,2,3],
             "Duration in Current address" : [''],
             "Most valuable available asset": [1,2,3,4],
             "Age" : [''],
             "Concurrent Credits" : [1,2,3],
             "Type of apartment" : [1,2,3],
             "No of Credits at this Bank" : [''],
             "Occupation" : [1,2,3,4],
             "No of dependents" : [''],
             "Telephone" : [1,2], 
             "Foreign Worker" : ['']}

  ### Constructing new table ###
  columns = [ col+"_"+str(label) if label!='' else col for col, labels in columns_dict.items() for label in labels ]
  data = np.zeros( (X.shape[0], len(columns)) )
  df = pd.DataFrame(data, columns=columns)

  # Adding features with dummy variables
  for col in [col for col, label in columns_dict.items() if len(label)>1]:
    dummy_variables = pd.get_dummies(X[col].values, prefix=col)
    df[dummy_variables.columns] = dummy_variables
  return df

def X_standardized(X):
  # standardizing the matrix so the mean for each feature is 0
  X_standardized = pd.DataFrame(preprocessing.scale(X), columns=X.columns)
  return X_standardized

class LogRegression:
  model = None 

  def fit(self, X, Y):
    X_dummies = add_dummies(X)
    X_norm = X_standardized(X_dummies)
    self.df = X_norm
    self.model = LogisticRegression().fit(X_norm, Y)

  def predict(self, X):
    X_dummies = add_dummies(X)
    X_norm = X_standardized(X_dummies)
    prediction = self.model.predict(X_norm)
    return prediction

"""**Logistic Regression**

Logistic regression is a linear combination of various features and is in practice very similar to linear regression. The main difference between the two models is that the predictor is not continuous but binary. To make a prediction, logistic regression assumes that there exists a linear relationship between the features and the log-odds of the prediction. The log-odds is often called the link function, and it allows us to predict an estimation as a linear relationship between the features despite the non-linear predictor variables. To determine the coefficients of the logistic regression numerical methods are used. Sklearn offers numerous algorithms to solve this optimization problem, and the one we will use is the lbfgs algorithm to find an optimum. By default logistic regression in sklearn implements Lasso regression with a regularization strength of 1. An essential characteristic of regularization is that it penalizes large coefficients and minimizes the risk of overfitting the data, and therefore we will adhere to the default settings.

To utilize categorical features, we have to implement dummy variables for each label in each categorical variable. This is done above with the method add dummies. The coefficients for these dummy variables take on a value of one or zero. consequently, depending on the assigned coefficients different models are proposed. 

**What are the hyperparameters?**

The main hyperparameter of the model is the regularization strength as it determines the extent large coefficients are penalized. Moreover, depending on the method used to solve the optimization problem different estimations are proposed and therefore, the choice of model also represents a hyperparameter. 

**What are the parameters and how to estimate them?**

To derive the associated coefficients one maximizes the log-likelihood of the coefficients given the data. Which result in the following equation:

$ l(B) = \sum^{n}_{i=1}[y_{i}log(\pi_{i})+(1-y_{i})log(1-\pi_{i})]$

$ l(B) = \sum^{n}_{i=1}[y_{i} log(\frac{\pi_{i}}{1-\pi_{i}})+log(1-\pi_{i})]$

$ l(B) = \sum^{n}_{i=1}[ y_{i}x_{i}\beta-log(1-e^{x_{i}\beta})]$

To include the penalty we have to add a term that penalizes large coefficients:

$ l(B) = \sum^{n}_{i=1}[ y_{i}x_{i}\beta-log(1-e^{x_{i}\beta})] - \lambda \sum^{p}_{j=1} \beta_j^2$

Note that this equation is not linear and thus analytical methods have to be used to find the maximum. Moreover, lambda which is often referred to as the regularization strength is a hyperparameter. To determine an appropriate value of lambda an iterative method is used.

### Naive Bayes Classifier
"""

class NBC:
  ### Variables ###
  columns_categories = ["Account Balance", "Payment Status of Previous Credit", "Purpose",
              "Value Savings/Stocks", "Length of current employment", "Instalment per cent", "Sex & Marital Status",
              "Guarantors", "Duration in Current address", "Most valuable available asset", "Concurrent Credits",
              "Type of apartment", "No of Credits at this Bank", "Occupation", "No of dependents", "Telephone", "Foreign Worker"]

  columns_numeric = ["Duration of Credit", "Credit Amount", "Age"]

  prior, prob_categories, prob_numeric = (None,None,None)

  def fit(self, X, Y):
    data = X.copy()
    data["Credibility"] = Y

    ### Prior ###
    prior = pd.Series(data = [(data["Credibility"] == 1).sum()/len(data),
                              (data["Credibility"] == 2).sum()/len(data)], index=[1,2])
    ### likelihood ###
    # calculate likelihood for categorical features 
    prob_categories = pd.DataFrame(data = np.zeros((2*11, len(self.columns_categories))),
                                   index = [[i for i in range(1,3) for _ in range(11)], [i for _ in range(2) for i in range(11)]],
                                   columns = self.columns_categories)
    for col in self.columns_categories:
      mask_pos = (data["Credibility"] == 1)
      mask_neg = (data["Credibility"] == 2)
      ds = 0.0001
      prob_categories.loc[(1,),col] = [ ((data.loc[mask_pos, col] == i).sum()+ds)/mask_pos.sum()  for i in range(11) ]
      prob_categories.loc[(2,),col] = [ ((data.loc[mask_neg, col] == i).sum()+ds)/mask_neg.sum()  for i in range(11) ]

    # calculate the likelihood for numerical features
    col = pd.MultiIndex.from_tuples( [(col,i) for i in range(1,3) for col in self.columns_numeric] )
    row = ["alpha","beta"]
    prob_numeric = pd.DataFrame(columns=col, index=row)

    for c in col: 
      alpha, beta = MLE_gamma(data.loc[(data["Credibility"] == c[1]),c[0]])
      prob_numeric.loc["alpha",c] = alpha
      prob_numeric.loc["beta",c] = beta

    self.prior, self.prob_categories, self.prob_numeric = prior,prob_categories,prob_numeric
    return prior,prob_categories,prob_numeric

  def calc(self, data, class_):
    L = self.prior[class_]
    for feature in data.index: 
      if(feature in self.columns_categories):
        p = self.prob_categories.loc[(class_, data[feature]), feature]
      if(feature in self.columns_numeric):
        alpha = self.prob_numeric.loc["alpha",(feature,class_)]
        beta = self.prob_numeric.loc["beta",(feature,class_)]
        x  = data[feature]
        p = stats.gamma.pdf(x,alpha, scale=beta)
      L += math.log(p)    
    return L

  def getClass(self, data):
    L1 = self.calc(data,1)
    L2 = self.calc(data,2)
    if L1>L2:
      return 1
    else:
      return 2

  def predict(self, data):
    predictions = []
    for row in data.index:
      predictions.append(self.getClass(data.loc[row,:]))
    return np.array(predictions)

"""The data set contains both categorical- and continuous variables and therefore, we have a combination of both Multinomial naive Bayes classifier and Gaussian naive Bayes classifier.

**Multinominal naive Bayes:**

The general mathematical expression of the Multinomial naive Bayes is written as: 

$P(Y = y | X = x) = \frac{P(X = x | Y = y)P(Y = y)}{P(X = x)}$


where $P(X = x | Y = y)$ is the likelihood function,

$P(Y = y)$ is the prior,

$P(X = x)$ is the normalization constant. 

If we translate these general expressions into our project, the prediction y is either good or bad credit, and the variables $x_{i}$ are a person with values in each feature, i = 1, …, n, where n is the number of features. The likelihood function is written as $P(features_{i,j}| good)$ and $P(features_ {i,j}| bad)$, where j = 1, …, m: m is the number of subcategories in each feature. The prior is written as $P(good)$ and $P(bad)$, and the normalization constant is written as $P(feature_{i,j})$. 

The purpose is to maximize the model, which can be expressed as:

$ŷ = arg max P(c) P(features_{i,j}| c)$, where $c ∈ {good, bad}$

**Gaussian naive Bayes:**

The general mathematical expression of the Gaussian naive Bayes is written as: 

$ P(Y = y | X = x) =  \frac{f_{X|Y = y}(x | Y = y)P(Y = y)}{f_{X}(x)} $


where $f_{X | Y = y}(x | Y = y)$ is the likelihood function, 

$P(Y = y)$ is the prior, 

$f_{X}(x)$ is the normalization constant. 

If we again translate these general expressions into our project, the prediction y is either good or bad credit, and the variables $x_{i}$ are the different features, i = 1, …, n, where n is the number of features. The prior is written as $P(good)$ and $P(bad)$, the likelihood function is written as $f_{i}(feature_{i} | good)$ and $f_{i}(feature_{i} | bad)$. 

The purpose is the same as for Multinomial naive Bayes - maximize the model, which can be expressed as: 

$ŷ = arg max P(c) ƒ_{i}(feature_{i} | c)$, $c ∈ {good, bad}$

**What are the hyperparameters?**

The hyperparameters for the Multinomial naive Bayes is the smoothing factor  $\alpha$. But for the Gaussian naive Bayes there are no hyperparameters. 

**What are the parameters and how to estimate them?**

For Multinomial naive Bayes, the parameters that are needed are the prior $P(good)$ and $P(bad)$, values from each category and the likelihood function for all given values.  

The prior $P(good)$ and $P(bad)$ is estimated by dividing the number of either good or bad credits by the total number of people/rows in the training data. The likelihood function is estimated by for each category i, we calculate the frequency of each subcategory j for both classes (good and bad credit). Then the probability is calculated for each subcategory by dividing the frequency with the total number of people/rows in each class (good or bad credit). 

For Gaussian naive Bayes, the parameters that are needed are the prior $P(good)$ and $P(bad)$, $ \mu_{c, i}$ and $\sigma_{c, i}$ in the likelihood function, where c = class good or bad, and i = each category. 

The prior $P(good)$ and $P(bad)$ is estimated the same way as in the Multinomial naive Bayes. $\mu_{c, i}$  is estimated by computing the sample mean for each class (good or bad credit) in each category. $ \sigma_{c, i}$ is estimated by computing the sample standard deviation for each class (good or bad credit) in each category.

### Random Forest
"""

class RandomForets: 
  model = None
  def fit(self, X, Y):
    self.model = RandomForestClassifier()
    self.model.fit(X, Y)

  def predict(self, X):
    return self.model.predict(X)

"""A third classifier included in this project, which is somewhat outside of the scope of the course both in, is the random forest classifier. If the opponent group want to disregard from it, we are more than fine with that since we only were expected to include two different ones, however, for the sake of learning we included this one as well.

**Random forest**


The random forest classifier belongs to a category of algorithms callse *ensemble learning algorithms*, which essentially means that it utilizes multiple smaller and separate learning algorithms in combination to increase the performance of the overall classifier. In this case, the classifier constructs multiple different decision trees during the training phase which initially are all being evaluated separately. In order to make a classification, the mode of all the separately evaluated trees is determined as the final decision for the aggregated forest. This aggregation of the separate trees is conducted to reduce the variance of a simple tree’s evaluation while still maintaining a classifier that still is rather unbiased. However, it also results in that there exist no generic mathematical expression for the classification per se, but instead only expressions for the different levels of parameter evaluations. 

**What are the parameters and how to estimate them?**


In the training phase, a number of features and rows are selected at random with replacement, which are evaluated each tree of the forest. The word evaluated refers to the act of calculating the importance of a particular node in the tree, using the following formula (using the Gini importance, more on that later):

$nodeImportance_{j} = w_{j}C_{j} - weight_{left(j)}C_{left(j)}-weight_{right(j)}C_{right(j)}$
 
where 

$w_{j}$ is the weighted number of samples that reaches node j
$C_{j}$ is the impurity value for node j

Thereafter, the feature importance for all features on a decision tree is calculated in accordance with the following formula:

$featureImportance_{i} = \frac{\sum_{j:node\:j\:splits\:on\:feature\:i}nodeImportance_{i}}{\sum_{k\in all\:nodes }nodeImportance_{k}}$

Usually, these feature importances are normalized before deciding the importance of all respective decision trees that make up the random forest. This last step of calculation the feature importance, at the forest level, is typically an averahe between all the number of trees in the forest, in accordance with the following:

$randomForestFeatureImportance_{i} = \frac{\sum_{j\in all\:trees}normalizedFeatureImportance_{ij}}{Total\:number\:of\:trees}$


Thus, the decision is made by starting with individual nodes in every decision tree and then working upwards for features, evaluation of the total tree and, lastly, evaluation of all the decision trees. 

**What are the hyperparameters?**


There exist some hyperparameters of the model, some of which can be adjusted to fine-tune it for the specific data set while other not have especially significant effects on the end-result. However, the most important hyperparameters to mention are *number of trees that the forest includes* and *criteria for which feature we should split on*. There exist many other hyperparameters, but these does not have major effects when assembling the individual trees in a random forest as long as the number of trees is sufficiently large. 


The *number of trees that the forest includes* simply determines how many trees the forest chooses to rely upon for the classification. The second mentioned hyperparameter, the *criteria for which feature we should split on*, is regulates what to take into consideration when computing the associated weights for each feature in the decision trees, and ultimately controls how the data should be split between the features. In the scikit-learn library utilized in this project, the Gini metric is used per default, however, one could consider other criteria such as entropy. The number of trees implemented in the model is also using the default value of the scikit-learn library of 100, since this value not meaningfully affects the operating result of the model as long as it is relatively large. 


Note, this description is very reduced and assumes that one have a basic understanding of how decision trees work. However, since the classifier lies somewhat in the outskirts of the course (and is considered additional since the two main classifiers above are mainly used), we will not explain everything here.

## Evaluating Models
"""

def get_estimates(confusion):
  cost_matrix = np.array([[1000, -1000],
                          [-5000, 0]])

  # Estimates
  precision = [ cm[0,0]/(cm[0,0] + cm[0,1]) for cm in confusion.values() ]
  recall = [cm[0,0]/(cm[0,0] + cm[1,0]) for cm in confusion.values() ]
  specificity = [cm[1,1]/(cm[1,1] + cm[0,1]) for cm in confusion.values()]
  f1 = [ 2*(p*r)/(p+r) for p,r in zip(precision, recall)]
  cost = [sum((x*cost_matrix).flatten()) for x in confusion.values()]
  metrics_df =    pd.DataFrame(data=[ [r,p,s,f,c] for f,r,p,s,c in zip(f1, recall, precision, specificity, cost)],
                              index=confusion.keys(),
                              columns=["Recall","Precison", "Specificity", "F1", "Cost"])
  return metrics_df

# Initiate models
Regression = LogRegression()
Nbc = NBC()
Randomforest = RandomForets()

# Train the models
Regression.fit(X_train,Y_train)
Nbc.fit(X_train, Y_train)
Randomforest.fit(X_train, Y_train)

# Predict the test sets using the trained models
Nbc_reg = Nbc.predict(X_test)
pred_reg = Regression.predict(X_test)
Random_reg = Randomforest.predict(X_test)

# Constructing confusion matrices 
# Confusion matrix whose i-th row and j-th column entry indicates the number of 
# Samples with true label being i-th class and prediced label being j-th class.
confusion = {
  "Regression" : confusion_matrix(Y_test, pred_reg, labels=[1,2]),
  "Nbc" : confusion_matrix(Y_test, Nbc_reg, labels=[1,2]),
  "Random" : confusion_matrix(Y_test, Random_reg, labels=[1,2])
}

metrics_df = get_estimates(confusion)

# Graph the matrices
fig, ax = plt.subplots(nrows=1,ncols=3)
max_ = max([max(x.flatten()) for x in confusion.values()])
min_ = min([min(x.flatten()) for x in confusion.values()])


cmap = plt.get_cmap("plasma")
cbar_ax = fig.add_axes([.91, .3, .03, .4])
for axis,data in zip(ax,confusion.items()):
  axis.set_title(data[0])
  sns.heatmap(data=data[1], ax=axis, annot=True, fmt='g',
              square=True, cbar_ax=cbar_ax, vmin=min_, vmax=max_)

plt.show()
metrics_df

"""To determine the most suitable model, we will have to compare them appropriately. The metrics that will be used to compare models are derived from the confusion matrix. Therefore a confusion matrix, together with several critical metrics calculated from the matrix has been constructed for each model. Notice that accuracy is not included in this analysis. The reason being that such a metric is unsuitable when analyzing unbalanced data. In addition to the more common metrics, we have also included a cost estimate associated with each model. This amount is calculated by an associated cost matrix  ($M_{Cost}$). The cost matrix links each outcome specified in the confusion matrix with a cost.

$
M_{Cost} = 
\begin{bmatrix}
1000 & -1000  \\
-5000 & 0  
\end{bmatrix}
$

From the result, one can observe that recall plays a more significant role than both precision and specificity when determining the cost. The main reason for this is bad customers that are granted credit incurs a much higher cost than the income that good customers result in. Consequently, the naive bais model results in the lowest cost while having the worst scores in precision, specificity, and F1 score. 

The goal for the bank is to decreases their cost and therefore, specificity, and cost will be the metrics that we will utilize when comparing models. Of these two metrics, specificity will be the most decisive metric as it influences cost most. One risk that exists with the cost estimate is that it is dependent on the dataset, and therefore, the conclusions may not be as generalizable as specificity. 
"""

features = DF_Original.iloc[:,0:-1]
classes = DF_Original.iloc[:,-1]

# Initiate models 
Regression = LogRegression()
Nbc = NBC()
Randomforest = RandomForets()

# Initiate k-fold algo
runs = 10
kf = KFold(n_splits=runs)
kf.get_n_splits(features)

# Initiating table for metrics
df = pd.DataFrame(columns = ["Recall",  "Precison", "Specificity",  "F1",   "Cost"],
                  index = [[i for i in range(runs) for _ in range(3)],
                          [i for _ in range(runs) for i in ["Regression","Nbc","Random"]]])

for idx, (train_index, test_index) in enumerate(kf.split(features)):
  # Retrieve indices
  X_train = features.loc[train_index,:].reset_index(drop=True)
  X_test = features.loc[test_index,:].reset_index(drop=True)
  Y_train = classes[train_index].reset_index(drop=True)
  Y_test = classes[test_index].reset_index(drop=True)

  # Fitting models
  Nbc.fit(X_train, Y_train)
  Randomforest.fit(X_train, Y_train)
  Regression.fit(X_train, Y_train)

  #constructing confusion matrix
  confusion = {
    "Regression" : confusion_matrix(Y_test, Regression.predict(X_test), labels=[1,2]),
    "Nbc" : confusion_matrix(Y_test, Nbc.predict(X_test), labels=[1,2]),
    "Random" : confusion_matrix(Y_test, Randomforest.predict(X_test), labels=[1,2])}

  df.loc[(idx,)] = get_estimates(confusion).values

# Visualizing differences 
# Selecting the data to be used
regression_df = df.iloc[df.index.get_level_values(1) == "Regression"]
nbc_df = df.iloc[df.index.get_level_values(1) == "Nbc"]
random_df = df.iloc[df.index.get_level_values(1) == "Random"]

# Plotting histograms 
fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(16, 8))
for idx, axis in enumerate(ax): 
  col = df.columns.values[idx]
  axis.hist(regression_df[col].values, color='k', bins=5, alpha=0.3, label="Regression")
  axis.axvline(regression_df[col].mean(), color='k', linestyle='dashed', linewidth=1)

  axis.hist(nbc_df[col].values, color='r', alpha=0.3, bins=5, label="NBC")
  axis.axvline(nbc_df[col].values.mean(), color='r', linestyle='dashed', linewidth=1)

  axis.hist(random_df[col].values, color='g', alpha=0.3, bins=5, label="Random")
  axis.axvline(random_df[col].values.mean(), color='g', linestyle='dashed', linewidth=1)

  axis.set_title(col)
  axis.legend(loc='upper left')
plt.tight_layout()

"""The figures above are the result of various metrics calculated from several training and validation sets. To optimize the cross-validation with the limited data we have used k-fold cross-validation, with 10 folds to partition the data. The result of each run is aggregated in the histograms shown above. The histogram shows the distribution of each metric for each model. The vertical dashed line represents the average score associated with each model. As indicated by the graph it seems that the Naive Bayes Classifier has a better recall score than the other models. Additionally, this model also results in a lower cost than the other models. However, it is not obvious that the difference between the Naive Bayes Classifier and the two other classifiers represents a significant difference. In order to say that the difference between the models is significant, we conduct a t-test."""

# T-test 
t_score_precison, p_value_precison = stats.ttest_rel(nbc_df["Precison"], random_df["Precison"])
t_score_cost, p_score_cost = stats.ttest_rel(nbc_df["Cost"], random_df["Cost"])
df_Ttest = pd.DataFrame(data = {"t_Score" : [t_score_precison, t_score_cost], "p_value" : [p_value_precison, p_score_cost]},
                        index = ["Precison", "Cost"])
df_Ttest

"""By doing a paired t-test, where the null hypothesis is that the average score is the same for the two best models we can answer whether or not the difference between the models are significant, and not just a random anomaly. Before conducting the t-test we decide upon an alpha of 5%. We conduct in total two paired t-tests, one associated with the precision, and another for the cost. As indicated by the result above, we reject the null hypothesis for both precision and cost as the associated p-value is less than our alpha level. Therefore, the difference between precision and cost between the two best models are significantly different with a confidence interval of 95%.

# Conclusion

The purpose of this project has been to investigate one approach for banks to better manage their credit risk by utilizing a predictive classifier instead of qualitative assessments. The underlying motivation is constituted by the fact that a more data-driven approach possibly can be configured to be less biased, increase the accuracy of decisions, and increase efficiency in the process. The investigation has found a suitable model for the particular situation, through analyzing both quantitative metrics and qualitative aspects of it.

Based on the insights above the NBC seems to represent the most appropriate model for this purpose. This conclusion is not solely based on the quantitative metrics recall and cost, but also takes into consideration the other presented ones. However, even though NBC performs worst in regards to these metrics they still uphold a value above 80%, which is above our requirement. Moreover, it should be noted that other perspectives than the measured metrics should be considered when choosing a model. For example, applicability and transparency is a critical perspective that most likely has to be included. For both the NBC and regression model a prediction could easily be analyzed by evaluating the likelihood and coefficients respectively. This transparency would probably be necessary both from an ethical and regulatory standpoint. Such transparency would also imply that these models are more understandable and thus more effortless for an organization to grasp. Consequently, it would probably be integrated and utilized throughout the organization much more smoothly than more complicated models would.

Moving forward, we suggest the next actionable task to be an improvement of the model in itself. This investigation has been essential to derive one model suitable for the application in a bank, however, it is still rather limited by the input data. One reasonable action when improving the model is to decrease the complexity while maintaining the predictive power. Here one could use the AIC and BIC metric to guide the choices. The methodology to find an appropriate set of features that maintains the predictive power while minimizing the complexity would most likely be iterative in nature. To increase the efficiency one ought to leverage the insights gained during the explorative phase of this project. During this phase, numerous highly correlated features were identified and several features seemed to be insignificant when predicting credibility, which was identified during the hypothesis testing. These insights represent the starting point when further developing the model and reducing the complexity. Thus, an analysis of the effect certain features has on the classification should be carried out to tune the model to the specific data of the implementing bank. Furthermore, since there exist many outliers in some of the variables, we also suggest future analysis to deal with these in order to determine how much they affect the model - and if there exist possibilities to improve the data gahtering in any way. 

Lastly, we suggest the bank implementing this model to consider which data that is attainable, but also ethically and lawfully usable in their own context. Certain features included in this analysis are unlawful to consider in certain countries, such as sex and marital status. This will affect the initial accuracy of the model. However, this investigation still provides a sound platform for the bank to stand upon when moving forward – and a good baseline for what they should be able to achieve by implementing a model instead of qualitative assessment of the customers. Ultimately, given that the data at the bank is not too skewed, this should result in a fairer assessment of customer credibility than the uncoordinated assessment performed without this model.

# Appendix: Estimate Parameters

This does really not be considered by the opponent if they are not interested in learning more about this. No guarantees that is is 100% right. Resistance is futile.

## The Quest for Gamma Parameters Without fit()

$ \alpha = shape \;\;\;\;\;\;\;\; \beta = rate  $

$f(x;\alpha ,\beta ) = \frac{\beta ^{\alpha }x^{\alpha -1}e^{-\beta x}}{\Gamma (\alpha)}$

$L(\alpha ,\beta) =  \coprod_{1}^{n} \frac{\beta ^{\alpha }x^{\alpha -1}e^{-\beta x}}{\Gamma (\alpha)}$

$L(\alpha ,\beta) = \frac{\beta ^{\alpha n} \prod( x^{\alpha -1})e^{-\beta \sum x}}{\Gamma (\alpha)^{n}}$

$\gamma = ln(L(\alpha ,\beta)) = ln(\beta) \alpha n+(\alpha-1)\sum ln(x)-\beta \sum x - ln(\Gamma(\alpha))n$


$f_{1}=\frac{\partial \gamma}{\partial \beta} = \frac{1}{\beta} \alpha n -\sum x$

$f_{1}=0 \Rightarrow   \beta = \frac{\alpha}{\bar{x}} $ 

$f_{2}=\frac{\partial \beta}{\partial \alpha } = ln(\beta) n +\sum ln(x)-n\frac{\Gamma(\alpha)}{\Gamma'(\alpha)}$

$f_{2}=\frac{\partial \beta}{\partial \alpha } = ln(\frac{\alpha}{\bar{x}}) n +\sum ln(x)-n\psi$

$f_{1}=f_{2}= 0$

$f_{1}$ Is a linear function and therefore it can easiliy be solve analytically. However $f_{2}$ is a non-linear function and therefore it has to be solved by a numerical method. In this case we will use the Gauss-Newton method. A quick note should be emphasized regarding the Beta parameter, namely that it is equal to the inverse of the scale parameter. The scale parameter is used in the stats library, and therefore we will take the inverse of beta to attain this parameter.  

$y_{k+1} = y_{k}- \frac{ f_{2}(k) }{ f_{2}'(k)}$

$f_{2} = ln(\alpha)n -ln(\bar{x})n +\sum ln(x)-n\psi(\alpha)$

$f_{2}'= \frac{n}{a} - n\psi'$

$y_{k} = [\alpha_{k}]$
"""

def MLE_gamma(x):
  # The tolerance and distance parameters are responsible for the accuracy of the approximation 
  tolerance = 0.00001
  distance = float('inf')
  # Variables needed to compute the new value
  p0 = 1
  n = len(x)
  mean = x.mean()
  log_sum = np.log(x).sum()

 # Iterative steps related to the Gauss-Newton method 
  while distance > tolerance:
    f_prim = n/p0-n*polygamma(1,p0)
    f = n*math.log(p0)-n*math.log(mean) +log_sum-n*digamma(p0)
    p_1 = p0 - f/f_prim
    distance = abs(p_1 - p0)
    p0 = p_1
  return p0, mean/p0

# Create plots 
fig, ax = plt.subplots(nrows=2, ncols=3,figsize=(10,10))

# Distributions 
ax[0][0].hist(DF_Original["Duration of Credit"])
ax[0][1].hist(DF_Original["Credit Amount"])
ax[0][2].hist(DF_Original["Age"])

# QQ-plots with estimates 
a,b = MLE_gamma(DF_Original["Duration of Credit"])
data = stats.gamma.rvs(a, scale=b, size=1000)
qq_x, qq_y = qq_plot(DF_Original["Duration of Credit"], data)
ax[1][0].scatter(qq_x, qq_y)

a,b = MLE_gamma(DF_Original["Credit Amount"])
data = stats.gamma.rvs(a, scale=b, size=1000)
qq_x, qq_y = qq_plot(DF_Original["Credit Amount"], data)
ax[1][1].scatter(qq_x, qq_y)


a,b = MLE_gamma(DF_Original["Age"])
data = stats.gamma.rvs(a, scale=b, size=1000)
qq_x, qq_y = qq_plot(DF_Original["Age"], data)
ax[1][2].scatter(qq_x, qq_y)


#plot variables 
plt.tight_layout()
plt.show()